{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - Classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()  \n",
    "relative_path = r\"..\\..\\..\\DataSets\\Rebanced_DataSets\\Random_DataSets\"\n",
    "dataset_dir = os.path.normpath(os.path.join(base_dir, relative_path)) \n",
    "print(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_folder(folder_path):\n",
    "    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "painting_path = os.path.join(dataset_dir, 'image', 'Painting')\n",
    "schematics_path = os.path.join(dataset_dir, 'image', 'Schematics')\n",
    "sketch_path = os.path.join(dataset_dir, 'image', 'Sketch')\n",
    "text_path = os.path.join(dataset_dir, 'image', 'text')\n",
    "photo_path = os.path.join(dataset_dir, 'Photo')\n",
    "\n",
    "\n",
    "num_paintings = count_images_in_folder(painting_path)\n",
    "num_schematics = count_images_in_folder(schematics_path)\n",
    "num_sketches = count_images_in_folder(sketch_path)\n",
    "num_text = count_images_in_folder(text_path)\n",
    "num_photos = count_images_in_folder(photo_path)\n",
    "\n",
    "print(f\"Paintings: {num_paintings}, Schematics: {num_schematics}, Sketches: {num_sketches}, Text: {num_text}, Photos: {num_photos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale (normalisation)\n",
    "\n",
    "Le paramètre rescale=1./255 normalise les valeurs des pixels de l'image en les divisant par 255. Cela signifie que les valeurs des pixels, qui sont normalement dans la plage de 0 à 255 (puisque les images sont généralement encodées en 8 bits), sont mises à l'échelle pour être comprises entre 0 et 1.\n",
    "\n",
    "et aussi un réechantillonage en focntion des poids des classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_generator = augmentation.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = augmentation.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(train_generator.class_indices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas, le déséquilibre entre \"Image\" et \"Photo\" est modéré (ce que semble indiquer nos poids de classe), nous allons commencer avec l'utilisation des poids de classe. C'est une solution simple et efficace dans la plupart des cas, surtout si l'écart entre les classes n'est pas trop important.\n",
    "\n",
    "Sinon après analyse des performances, si nous constatons que la classe minoritaire n'est toujours pas bien apprise, nous pouvez alors envisager d'ajouter un rééchantillonnage pour cette classe en appliquant des techniques de data augmentation sur les images de la classe \"Image\".\n",
    "redimensionnement et orientation différente de s image (ex 18 degres, ect.. workshop 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Rescaling(1./255, input_shape=(128, 128, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.random.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Fonction de prédiction avec probabilités\n",
    "def predict_image_binary_with_probabilities(model_save, image_path):\n",
    "    img = load_img(image_path, target_size=(128, 128))  # Adapter la taille à celle du modèle\n",
    "    img_array = img_to_array(img)  # Conversion en tableau numpy\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Ajouter la dimension batch\n",
    "    img_array /= 255.0  # Normalisation de l'image\n",
    "\n",
    "    # Prédiction binaire (probabilité unique)\n",
    "    predictions = model_save.predict(img_array)\n",
    "    probability = predictions[0][0]  # Probabilité que l'image soit une \"Photo\"\n",
    "    \n",
    "    # Classification basée sur la probabilité\n",
    "    if probability > 0.7:\n",
    "        predicted_class = 'Photo'\n",
    "    else:\n",
    "        predicted_class = 'Not-Photo'  # \"Image\" comme Not-Photo dans ce cas\n",
    "\n",
    "    # Retourner la classe prédite et les probabilités (classe photo et non-photo)\n",
    "    return predicted_class, [probability, 1 - probability]\n",
    "\n",
    "# Fonction d'affichage des images avec probabilités\n",
    "def display_images_with_binary_probabilities(model_save, dataset_dir, num_images=6):\n",
    "    subfolders = ['Painting', 'Schematics', 'Sketch', 'Text']  # Liste des sous-dossiers d'images\n",
    "    folder = [\"Image\",\"photo\"]\n",
    "    fig, axes = plt.subplots(2, num_images//2, figsize=(18, 8))  # Créer une grille pour afficher les images\n",
    "\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']  # Extensions d'images valides\n",
    "\n",
    "    for i in range(num_images):\n",
    "        randomfolder = random.choice(folder) \n",
    "        if randomfolder == \"photo\":\n",
    "             image_folder = os.path.join(dataset_dir, randomfolder) \n",
    "        else:\n",
    "            subfolder = random.choice(subfolders)\n",
    "            image_folder = os.path.join(dataset_dir, randomfolder, subfolder)  \n",
    "\n",
    "        # Vérifier l'existence du sous-dossier\n",
    "        if not os.path.exists(image_folder):\n",
    "            print(f\"Folder not found: {image_folder}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Fichiers valides dans le sous-dossier\n",
    "            valid_files = [f for f in os.listdir(image_folder) if os.path.splitext(f)[1].lower() in valid_extensions]\n",
    "            \n",
    "            if len(valid_files) == 0:\n",
    "                print(f\"No valid image files found in: {image_folder}\")\n",
    "                continue\n",
    "\n",
    "            random_image = random.choice(valid_files)\n",
    "            image_path = os.path.join(image_folder, random_image)\n",
    "\n",
    "            # Charger l'image et obtenir la prédiction\n",
    "            img = load_img(image_path, target_size=(256, 256))  # Pour une meilleure visibilité\n",
    "            predicted_class, probabilities = predict_image_binary_with_probabilities(model_save, image_path)\n",
    "\n",
    "            # Sélectionner l'emplacement dans la grille (axes)\n",
    "            ax = axes[i // (num_images // 2), i % (num_images // 2)]\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Afficher la classe prédite et le pourcentage\n",
    "            probability_photo = probabilities[0] * 100  # Probabilité que ce soit une \"Photo\"\n",
    "            ax.set_title(f'Predicted Class: {predicted_class} ({probability_photo:.2f}%)', fontsize=12)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            ax.text(0.5, 0.5, 'Image Not Found', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_with_binary_probabilities(model, dataset_dir, num_images=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation des Poids Sauvegarder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_weights = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_weights.add(layers.Rescaling(1./255, input_shape=(128, 128, 3)))\n",
    "model_save_weights.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model_save_weights.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_save_weights.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_save_weights.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_save_weights.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model_save_weights.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_save_weights.add(Flatten())\n",
    "model_save_weights.add(Dense(128, activation='relu'))\n",
    "model_save_weights.add(Dropout(0.4)) \n",
    "model_save_weights .add(Dense(64, activation='relu'))\n",
    "\n",
    "\n",
    "model_save_weights.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_weights.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_weights.load_weights('model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_with_binary_probabilities(model_save_weights, dataset_dir, num_images=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
