{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Leyenda - Deliverable 1**\n",
    "\n",
    "*Group* : \n",
    "|Author|Center|Promo|Mail| \n",
    "|---|---|---|---|\n",
    "|De Jesus Correia Alexandre|Nanterre|FISE INFO A5|alexandre.dejesuscorreia@viacesi.fr|\n",
    "|Charlut Steven|Nanterre|FISE INFO A5|steven.charlut@viacesi.fr|\n",
    "|Debraize Killian|Nanterre|FISE INFO A5|killian.debraize@viacesi.fr|\n",
    "|Raies Youssef|Nanterre|FISE INFO A5|youssef.raies@viacesi.fr|\n",
    "|Kinya Mwongera Sharon|Nanterre|FISE INFO A5|sharon.kinyamwongera@viaceis.fr|\n",
    "\n",
    "![DeepLearning](https://i.pinimg.com/550x/4f/a5/54/4fa5542fadd6a9ee2e7f995b981934f6.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project**\n",
    "\n",
    "## Context :\n",
    "\n",
    "**TouNum**, a company specializing in the digitization of documents (text, images, etc.), is looking to expand its services to include 'Machine Learning' tools. Their current focus is on providing solutions for companies with large volumes of documents that need digitization, and they want to offer a service for automatic image categorization. Although TouNum has experience in digitization, they lack expertise in Machine Learning and have turned to CESI Data Science specialists to develop a solution. The proposed solution aims to analyze and describe images (captioning) in an automated way.\n",
    "\n",
    "In addition to this core goal, there are two main challenges:\n",
    "- **`Image Cleaning`** : Due to variations in image quality (e.g., blurriness, noise), a cleaning/pre-processing step is required before analyzing the images.\n",
    "- **`Image Classification`** : Since many of the digitized images are not actual photos but could be documents, diagrams, sketches, or paintings, there needs to be an initial classification step to separate photographs from other types of images.\n",
    "\n",
    "Luckily, TouNum has a dataset with thousands of categorized and labeled images, which can be used for supervised learning to train the necessary models.\n",
    "\n",
    "## Objectives :\n",
    "\n",
    "The project has three main objectives that will lead to a fully automated image analysis and captioning solution:\n",
    "- **`Binary Classification`** :\n",
    "Develop a neural network model using TensorFlow and SciKit to classify images into two categories—photos and non-photos (e.g., scanned documents, diagrams, paintings). Ideally, the model should differentiate between photos and other types of images, including drawings or paintings. This module will involve image pre-processing and the use of convolutional neural networks (CNN).\n",
    "- **`Image Processing`** :\n",
    "Implement image cleaning and pre-processing techniques using simple convolution filters to enhance the quality of the images before running further analyses. This step ensures that images are clear enough for subsequent classification and captioning tasks.\n",
    "- **`Image Captioning`** :\n",
    "Build a captioning model that automatically generates descriptive captions for images. This will require using a combination of convolutional neural networks (CNNs) for image processing and recurrent neural networks (RNNs) to generate textual descriptions. The model will be trained using classical datasets for image captioning.\n",
    "\n",
    "The final solution should be presented as a reproducible workflow (using Jupyter notebooks), and the prototype must be ready for deployment within five weeks. The workflow should be scalable and adaptable to any image data and include clear documentation to ensure maintenance and further development.\n",
    "\n",
    "## Tools : \n",
    "\n",
    "For our project, we will utilize **`Python`** as our programming language. Python offers libraries, such as NumPy and Pandas, that facilitate efficient data manipulation and analysis, while libraries like Scikit-learn and TensorFlow provide robust tools for implementing machine learning (ML) and deep learning (DL) algorithms.\n",
    "\n",
    "**`TensorFlow`**, a powerful open-source framework designed for building and deploying ML and DL models, will be our main project tool.\n",
    "It operates on the concept of 'tensors', which are multi-dimensional arrays that allow for efficient data representation and manipulation. The term 'flow' refers to the way data moves through a computational graph, where operations are represented as nodes and tensors as edges. This architecture enables TensorFlow to optimize the performance of complex mathematical computations, making it ideal for tasks such as neural network training and inference.\n",
    "\n",
    "TensorFlow provides a rich ecosystem with high-level APIs, such as **`Keras`**, for quick model prototyping, along with lower-level operations for fine-tuning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deliverable 1 : Binary Classification**\n",
    "\n",
    "TouNum aims to automate the selection of photos from a dataset that contains various types of images, such as scanned text, drawings, diagrams, and paintings. The ultimate goal is to filter out images that are not photos in preparation for captioning, which will be addressed in the next phase of the project. To achieve this, the deliverable will involve building and training a neural network model using TensorFlow to differentiate between photos and non-photo images.\n",
    "\n",
    "Given the wide variety of image types in the dataset, the challenge lies in **creating a robust model that can reliably distinguish photos from these other categories**. The approach should start with simpler distinctions and gradually incorporate more challenging tasks, such as differentiating realistic paintings from actual photos.\n",
    "\n",
    "The deliverable will include a detailed documentation of the neural network's architecture, the training process, and the resulting performance. The performance metrics will also be thoroughly analyzed to ensure the model strikes a balance between bias and variance and avoids common pitfalls like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Theory : \n",
    "\n",
    "#### Perceptron : \n",
    "A **'Perceptron'** is a type of artificial neuron, a basic unit in neural networks, designed to simulate the behavior of biological neurons. \n",
    "\n",
    "It takes several `input values`, applies `weights` to each of these inputs, `sums` them up with a `bias` (additional parameter that helps adjust the output), and passes the result through an `activation function`. If the result exceeds a certain threshold, the perceptron \"fires\", meaning it produces an `output` (typically 1; otherwise, the output is 0). This process where the input data is passed through the network’s layers to generate an output is called **`Forward Propagation`**.\n",
    "\n",
    "![Perceptron](https://miro.medium.com/v2/resize:fit:940/1*6HtyqOTYyYUzUeERVXL8Ag.png)\n",
    "\n",
    "`Weights` are parameters that determines the strength and direction of the connection between neurons across layers. It influences how much influence an input or neuron’s output has on the following layer, and during training, the model adjusts these weights to minimize errors and improve its predictions.\n",
    "\n",
    "An `activation function` defines how the output of a neuron is calculated based on its input, **introducing non-linearity into the model**. Common examples include :\n",
    "- the **sigmoid function**, which outputs values between 0 and 1, \n",
    "- the **ReLU (Rectified Linear Unit)**, which outputs 0 for negative inputs and the input itself for positive values,\n",
    "- the **tanh function**, which maps inputs to values between -1 and 1,\n",
    "- the **softmax function**, which converts raw outputs into probabilities on classes (and normalizing them so that the sum of all probabilities equals 1).\n",
    "\n",
    "These functions are crucial in neural networks because it introduces non-linearity, allowing the model to learn and represent complex patterns and relationships beyond simple linear combinations of inputs, so we can solve non linear problem.\n",
    "\n",
    "This simple decision-making process enables a perceptron to classify data into two categories, making it the foundation of binary classifiers. However, on its own, a single perceptron is limited in its ability to solve complex problems, as it can only classify linearly separable data.\n",
    "\n",
    "#### Deep Neural Network :\n",
    "A **'Deep Neural Network'** is an advanced structure composed of multiple layers of interconnected neurons (or perceptrons). \n",
    "\n",
    "Instead of just a single perceptron making decisions, in a deep neural network, there are several `hidden layers` between the `input player` and the `output layer`. Each layer processes data through many neurons, each performing a similar function to a perceptron. The network's depth allows it to capture more complex patterns and hierarchical representations from the data. The neurons in each layer take in the weighted sum of inputs from the previous layer, pass them through activation functions (like ReLU or sigmoid), and then send the output to the next layer.\n",
    "\n",
    "![DNN](https://www.researchgate.net/profile/Chuan-Lin-3/publication/333567419/figure/fig4/AS:765686351671297@1559565260413/Construction-of-the-deep-neural-network-DNN-model.jpg)\n",
    "\n",
    "`Hidden layers` in a DNN can include :\n",
    "- **Dense layers** (*or Fully connected layers*) : connect every neuron in one layer to every neuron in the next, allowing the network to capture global patterns in the data, \n",
    "- **Convolutional layers** : apply filters to detect local features like edges or textures, \n",
    "- **Dropout layers** : randomly deactivate a portion of neurons during training to prevent overfitting, improving generalization, \n",
    "- **Batch normalization layers** : normalize the inputs to each layer, speeding up training and improving stability, \n",
    "- **Pooling layers** : reduce the spatial dimensions of feature maps by summarizing nearby values, which helps to reduce computational complexity and makes the model more robust to small translations in the input. The most common types are ***Max Pooling***, which selects the maximum value from a region, and ***Average Pooling***, which takes the average of the values in that region.\n",
    "\n",
    "Through this process, the network can model intricate relationships and solve highly non-linear problems. Deep neural networks have become essential in tasks like image recognition, natural language processing, and other sophisticated AI applications due to their capacity to learn complex patterns through multiple layers of abstraction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
