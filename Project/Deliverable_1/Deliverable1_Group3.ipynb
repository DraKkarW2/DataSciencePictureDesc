{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Leyenda - Deliverable 1**\n",
    "\n",
    "*Group* : \n",
    "|Author|Center|Promo|Mail| \n",
    "|---|---|---|---|\n",
    "|De Jesus Correia Alexandre|Nanterre|FISE INFO A5|alexandre.dejesuscorreia@viacesi.fr|\n",
    "|Charlut Steven|Nanterre|FISE INFO A5|steven.charlut@viacesi.fr|\n",
    "|Debraize Killian|Nanterre|FISE INFO A5|killian.debraize@viacesi.fr|\n",
    "|Raies Youssef|Nanterre|FISE INFO A5|youssef.raies@viacesi.fr|\n",
    "|Kinya Mwongera Sharon|Nanterre|FISE INFO A5|sharon.kinyamwongera@viaceis.fr|\n",
    "\n",
    "![DeepLearning](https://i.pinimg.com/550x/4f/a5/54/4fa5542fadd6a9ee2e7f995b981934f6.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project**\n",
    "\n",
    "## Context :\n",
    "\n",
    "**TouNum**, a company specializing in the digitization of documents (text, images, etc.), is looking to expand its services to include 'Machine Learning' tools. Their current focus is on providing solutions for companies with large volumes of documents that need digitization, and they want to offer a service for automatic image categorization. Although TouNum has experience in digitization, they lack expertise in Machine Learning and have turned to CESI Data Science specialists to develop a solution. The proposed solution aims to analyze and describe images (captioning) in an automated way.\n",
    "\n",
    "In addition to this core goal, there are two main challenges:\n",
    "- **`Image Cleaning`** : Due to variations in image quality (e.g., blurriness, noise), a cleaning/pre-processing step is required before analyzing the images.\n",
    "- **`Image Classification`** : Since many of the digitized images are not actual photos but could be documents, diagrams, sketches, or paintings, there needs to be an initial classification step to separate photographs from other types of images.\n",
    "\n",
    "Luckily, TouNum has a dataset with thousands of categorized and labeled images, which can be used for supervised learning to train the necessary models.\n",
    "\n",
    "## Objectives :\n",
    "\n",
    "The project has three main objectives that will lead to a fully automated image analysis and captioning solution:\n",
    "- **`Binary Classification`** :\n",
    "Develop a neural network model using TensorFlow and SciKit to classify images into two categories—photos and non-photos (e.g., scanned documents, diagrams, paintings). Ideally, the model should differentiate between photos and other types of images, including drawings or paintings. This module will involve image pre-processing and the use of convolutional neural networks (CNN).\n",
    "- **`Image Processing`** :\n",
    "Implement image cleaning and pre-processing techniques using simple convolution filters to enhance the quality of the images before running further analyses. This step ensures that images are clear enough for subsequent classification and captioning tasks.\n",
    "- **`Image Captioning`** :\n",
    "Build a captioning model that automatically generates descriptive captions for images. This will require using a combination of convolutional neural networks (CNNs) for image processing and recurrent neural networks (RNNs) to generate textual descriptions. The model will be trained using classical datasets for image captioning.\n",
    "\n",
    "The final solution should be presented as a reproducible workflow (using Jupyter notebooks), and the prototype must be ready for deployment within five weeks. The workflow should be scalable and adaptable to any image data and include clear documentation to ensure maintenance and further development.\n",
    "\n",
    "## Software Requirements :\n",
    "\n",
    "For our project, we will utilize **`Python`** as our programming language. Python offers libraries, such as NumPy and Pandas, that facilitate efficient data manipulation and analysis, while libraries like Scikit-learn and TensorFlow provide robust tools for implementing machine learning (ML) and deep learning (DL) algorithms.\n",
    "\n",
    "**`TensorFlow`**, a powerful open-source framework designed for building and deploying ML and DL models, will be our main project tool.\n",
    "It operates on the concept of 'tensors', which are multi-dimensional arrays that allow for efficient data representation and manipulation. The term 'flow' refers to the way data moves through a computational graph, where operations are represented as nodes and tensors as edges. This architecture enables TensorFlow to optimize the performance of complex mathematical computations, making it ideal for tasks such as neural network training and inference. \n",
    "\n",
    "Moreover, TensorFlow provides a rich ecosystem with high-level APIs, such as **`Keras`**, for quick model prototyping, along with lower-level operations for fine-tuning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deliverable 1 : Binary Classification**\n",
    "\n",
    "TouNum aims to automate the selection of photos from a dataset that contains various types of images, such as scanned text, drawings, diagrams, and paintings. The ultimate goal is to filter out images that are not photos in preparation for captioning, which will be addressed in the next phase of the project. To achieve this, the deliverable will involve building and training a neural network model using TensorFlow to differentiate between photos and non-photo images.\n",
    "\n",
    "Given the wide variety of image types in the dataset, the challenge lies in **creating a robust model that can reliably distinguish photos from these other categories**. The approach should start with simpler distinctions and gradually incorporate more challenging tasks, such as differentiating realistic paintings from actual photos.\n",
    "\n",
    "The deliverable will include a detailed documentation of the neural network's architecture, the training process, and the resulting performance. The performance metrics will also be thoroughly analyzed to ensure the model strikes a balance between bias and variance and avoids common pitfalls like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Deep Neural Network (DNN) model we are going to implement is based on a fundamental principle: **images are represented as pixel matrices**, where each pixel is defined by a *light intensity value (grayscale)* or a *combination of values (colours)*. Our goal is to develop a model capable of analyzing these matrices and learning how to **recognize characteristic patterns** to distinguish between photos and images.\n",
    "\n",
    "Implementing a simple model based on dense layers (Fully Connected Layers) seems interesting : these layers will take each pixel as an individual input and try to identify global correlations between the pixels to determine whether an image is a photo or not. However, these dense layers are not optimal for analyzing images because they do not take into account **local relationships between neighbouring pixels**, which is crucial to understanding visual patterns.\n",
    "\n",
    "That is why we will introduce layers of **convolution** (Convolutional Neural Networks or CNN). Unlike dense layers, convolutional layers are specifically designed to extract local features by applying **filters** to areas of the image. These filters allow you to detect patterns such as edges, textures, and other visual characteristics, while maintaining the spatial structure of the image. By accumulating several layers of convolution, the model will be able to gradually learn increasingly complex features, making it easier to distinguish between a photo and non-photographic images such as drawings, diagrams or paintings.\n",
    "\n",
    "In addition, we will also use an **autocoder** : a type of unsupervised neural network designed to learn a compressed representation of input data (here, images), while retaining their main characteristics. This approach could be useful for extracting common underlying patterns, without the need for explicit labels. Once the autoencoder is trained, the resulting latent representation can then be used as input for a final classification model, thus allowing better differentiation of photos from other types of images. This process has the advantage of capturing complex and discrete features of images, often difficult to identify by traditional methods.\n",
    "\n",
    "The model’s drive will be applied to the provided dataset, a set of tagged images (photos, paintings, ...), and let the model adjust its parameters (weight and bias) to minimize prediction error. During this phase, it will be essential to monitor key indicators in order to avoid phenomena of **overfitting** (the model learns too well the details of the training game, but generalizes badly to new images) or of**underfitting** (the model does not capture the essential characteristics well).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Understanding Neural Networks \n",
    "\n",
    "### A.1. Perceptron : \n",
    "A **'Perceptron'** is a type of artificial neuron, a basic unit in neural networks, designed to simulate the behavior of biological neurons. \n",
    "\n",
    "It takes several `input values`, applies `weights` to each of these inputs, `sums` them up with a `bias` (additional parameter that helps adjust the output), and passes the result through an `activation function`. If the result exceeds a certain threshold, the perceptron \"fires\", meaning it produces an `output` (typically 1; otherwise, the output is 0). This process where the input data is passed through the network’s layers to generate an output is called **`Forward Propagation`**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:940/1*6HtyqOTYyYUzUeERVXL8Ag.png\" width=\"600\">\n",
    "\n",
    "`Weights` are parameters that determines the strength and direction of the connection between neurons across layers. It influences how much influence an input or neuron’s output has on the following layer, and during training, the model adjusts these weights to minimize errors and improve its predictions.\n",
    "\n",
    "An `activation function` defines how the output of a neuron is calculated based on its input, **introducing non-linearity into the model**. Common examples include :\n",
    "- the **sigmoid function**, which outputs values between 0 and 1, \n",
    "- the **ReLU (Rectified Linear Unit)**, which outputs 0 for negative inputs and the input itself for positive values,\n",
    "- the **tanh function**, which maps inputs to values between -1 and 1,\n",
    "- the **softmax function**, which converts raw outputs into probabilities on classes (and normalizing them so that the sum of all probabilities equals 1).\n",
    "\n",
    "These functions are crucial in neural networks because it introduces non-linearity, allowing the model to learn and represent complex patterns and relationships beyond simple linear combinations of inputs, so we can solve non linear problem.\n",
    "\n",
    "This simple decision-making process enables a perceptron to classify data into two categories, making it the foundation of binary classifiers. However, on its own, a single perceptron is limited in its ability to solve complex problems, as it can only classify linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Deep Neural Network :\n",
    "\n",
    "A **`Deep Neural Network`** is an advanced structure composed of multiple layers of interconnected neurons (or *perceptrons*). \n",
    "\n",
    "Instead of just a single perceptron making decisions, in a deep neural network, there are several `hidden layers` between the `input layer` and the `output layer`. Each layer processes data through many neurons, each performing a similar function to a *perceptron*. The network's depth allows it to capture more complex patterns and hierarchical representations from the data. The neurons in each layer take in the weighted sum of inputs from the previous layer, pass them through activation functions (like ReLU or sigmoid), and then send the output to the next layer.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Chuan-Lin-3/publication/333567419/figure/fig4/AS:765686351671297@1559565260413/Construction-of-the-deep-neural-network-DNN-model.jpg\" width=\"600\">\n",
    "\n",
    "The `input layer` is the first layer of the neural network : its primary role is to accept the raw data and pass it to the subsequent layers.\n",
    "\n",
    "The `output layer` is the final layer of the neural network : its purpose is to produce the final prediction or classification.\n",
    "\n",
    "`Hidden layers` perform computations and transformations on the data. These layers are called hidden because their outputs are not directly visible (internal to the network). It can include :\n",
    "- **Dense layers** (*or Fully connected layers*) : connect every neuron in one layer to every neuron in the next, allowing the network to capture global patterns in the data, \n",
    "- **Convolutional layers** : apply filters to detect local features like edges or textures, \n",
    "- **Dropout layers** : randomly deactivate a portion of neurons during training to prevent overfitting, improving generalization, \n",
    "- **Batch normalization layers** : normalize the inputs to each layer, speeding up training and improving stability, \n",
    "- **Pooling layers** : reduce the spatial dimensions of feature maps by summarizing nearby values, which helps to reduce computational complexity and makes the model more robust to small translations in the input. The most common types are ***Max Pooling***, which selects the maximum value from a region, and ***Average Pooling***, which takes the average of the values in that region.\n",
    "\n",
    "Through this process, the network can model intricate relationships and solve highly non-linear problems. Deep neural networks have become essential in tasks like image recognition, natural language processing, and other sophisticated AI applications due to their capacity to learn complex patterns through multiple layers of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3. Neurons & Tensors\n",
    "\n",
    "A **`neuron`** is a fundamental computational unit that processes input data :\n",
    "- The neurons in the input layer do not perform any computation : it simply forwards the input data to the next layer. Each neuron here represents a feature vector of the input data.\n",
    "- The neurons in the hidden layers compute a weighted sum of the inputs : each input is associated with a weight that determines its significance.\n",
    "- The neurons in the output layer produce the final prediction (or classification).\n",
    "\n",
    "In our model, the **inputs** are the dataset's images. When processed by a neural network, these images are converted into **numerical matrices** that represent pixel values. \n",
    "However, neural networks (especially in TensorFlow), essentially manipulate **tensors** throughout training and predictions.\n",
    "\n",
    "A **`tensor`** is a generalized data structure that can be seen as *multi-dimensional arrays of numbers* that represent complex data. For example, a one-dimensional vector is referred to as a 1D tensor, while a two-dimensional matrix is called a 2D tensor. In the case of a color image, a tensor would be a 3D array : height, width, and depth (corresponding to the three color channels : red, green and blue).\n",
    "\n",
    "Hence, each image need to be converted into a tensor, and will then be passed through the network to be analyzed and classified. \n",
    "\n",
    "<img src=\"https://hkilter.com/images/7/7a/Tensors.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Networks for Image Data\n",
    "\n",
    "### B.1. Image Data\n",
    "\n",
    "A `digital image` is made of pixels, which are the smallest elements of an image. \n",
    "\n",
    "In a black-and-white image, each pixel is represented by a number ranging from 0 (completely dark) to 255 (completely bright). These numbers indicate the intensity of light at that pixel. This kind of image data is the simplest form, often referred to as a grayscale image, because it only contains information about the brightness, not color.\n",
    "\n",
    "In color images, each pixel is represented by three numbers instead of one. These three numbers correspond to the primary colors of light: Red, Green, and Blue (RGB). Each of these values also ranges from 0 to 255, where 0 means no contribution of that color and 255 means the maximum contribution. By combining different levels of red, green, and blue, we can create any color.\n",
    "\n",
    "So, whether an image is black-and-white or color, the data underlying an image consists of a large grid of numerical values. These numbers form what we call **`image data`**, and this data is essential for computers to process, manipulate, or analyze images in tasks like image recognition, editing, or machine learning.\n",
    "\n",
    "***Example** : the image data for an image \"8\" from the MNIST Dataset* \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:794/0*9iwH0ebD6mQAUFt8\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**`Convolutional Neural Networks`** are specialized for image data. They excel in identifying spatial hierarchies and features within visual information.\n",
    "\n",
    "CNNs utilize a special type of \"hidden\" layer, the **convolutional layer**, that makes them well-positioned to learn from image and image-like data. Regarding image data, CNNs can be used for many different computer vision tasks, such as `image processing`, `classification`, `segmentation`, and `object detection`.\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/568241-4.png\"  width=\"600\">\n",
    "\n",
    "The `input layer` represents the input image. Because we use RGB images as input, the input layer has three channels, corresponding to the red, green, and blue channels respectively, which are shown in this layer. \n",
    "\n",
    "The `convolutional layers` are the foundation of CNN, as they contain the `learned kernels`, which extract features that distinguish different images from one another (this is particularly important for classification). \n",
    "\n",
    "This layer performs a dot product between two matrices, where one matrix is the set of learnable parameters, the **`kernel`** or filter, and the other matrix is the restricted portion of the receptive field. The kernel is spatially smaller than an image, but is more in-depth. This means that, if the image is composed of three (RGB) channels, the kernel height and width will be spatially small, but the depth extends up to all three channels.\n",
    "\n",
    "During the forward pass, the kernel slides across the height and width of the image, producing the image representation of that receptive region. This produces a two-dimensional representation of the image (the output, known as an **activation map**) that gives the response of the kernel at each spatial position of the image.\n",
    "\n",
    "Like others hidden layers, after the convolution operation, an activation function (commonly the ReLU function) is applied to introduce non-linearity.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*ulfFYH5HbWpLTIfuebj5mQ.gif\"  width=\"500\">\n",
    "\n",
    "`Hyperparameters of the Kernels` : \n",
    "- **Padding** : it conserves data at the borders of activation maps, which leads to better performance, and it can help preserve the input's spatial size, which allows an architecture designer to build deeper, higher performing networks. There exist many padding techniques, but the most commonly used approach is **zero-padding**, because of its performance, simplicity, and computational efficiency : the technique involves adding zeros symmetrically around the edges of an input.\n",
    "- **Size** : refers to the dimensions of the sliding window over the input (often 3x3 or 5x5). Choosing this hyperparameter has a massive impact on the image classification task. For example, small kernel sizes are able to extract a much larger amount of information containing highly local features from the input. Then, a smaller kernel size also leads to a smaller reduction in layer dimensions, which allows for a deeper architecture. Conversely, a large kernel size extracts less information, which leads to a faster reduction in layer dimensions, often leading to worse performance. Large kernels are better suited to extract features that are larger.\n",
    "- **Stride** : indicates how many pixels the kernel should be shifted over at a time. The impact stride has on a CNN is similar to kernel size. As stride is decreased, more features are learned because more data is extracted, which also leads to larger output layers. On the contrary, as stride is increased, this leads to more limited feature extraction and smaller output layer dimensions.\n",
    "\n",
    "The `pooling layers` have the purpose of gradually decreasing the spatial extent of the network, which reduces the parameters and overall computation of the network. There are many types of pooling layers in different CNN architectures, but `Max pooling` is the most popular technique (where the maximum value from a specific region of the feature map is taken).\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*sK7oP1m129V_oNGSsHIm_w.png\"  width=\"500\">\n",
    "\n",
    "A `flatten layer` can also be necessary : it converts a multi-dimensional layer into a one-dimensional vector to fit the input of a fully-connected layer for classification. For example, a 5x5x2 tensor would be converted into a vector of size 50. Particularly, some activation function (like the softmax function in dense layers) requires a 1-dimensional input.\n",
    "\n",
    "<img src=\"https://www.megabagus.id/wp-content/uploads/Machine_Learning/deep_learning/CNN/flattening.jpg\"  width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3. Image Classification \n",
    "\n",
    "The early applications of neural networks are image classification. \n",
    "\n",
    "For example, the digit recognition for ‘0’ to ‘9’ given various hand-written images. A trained neural network model can take an unknown digital image data to predict the probabilities for the multi-class output “0” to “9”. All these probabilities will then sum up to 1.0.\n",
    "\n",
    "Likewise, if we feed a neural network with various kinds of images with their corresponding labels (images and photos), the neural network can be trained to tell the likelihood for a new image. \n",
    "\n",
    "For an algorithm to recognize images in a manner comparable to human recognition, it must undergo a similar learning process. We achieve this by training the algorithm with thousands of labeled images (some labeled “photos”, and other types). During this training, the model learns to discern specific features that differentiate images from photos. Once trained, the model can analyze new, unlabeled images and determine whether they depict a photo or an image based on the features it has learned. \n",
    "\n",
    "***Example :** Image classification with animal labels* \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:786/format:webp/0*sVuvA9R9x8s3_EJ_.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4. AutoEncoders (AE)\n",
    "\n",
    "An **`autoencoder`** is a specialized type of neural network designed **to replicate its input values as its output values**. Unlike neural networks which are generally categorized under supervised learning, autoencoders are categorized under **unsupervised learning**, because they **do not need additional labeled targets**. The key interest in autoencoders lies not in the output layer (because it is simply a copy of the input layer) but in the **hidden layers**.\n",
    "\n",
    "An autoencoder should have fewer neurons in the hidden layers than that of the input layers. Because the hidden layers, which have fewer neurons than the input layers, must extract and retain only the most important features of the input data, the autoencoder will be forced **to compress the data into a lower-dimensional representation**, capturing the essential patterns while filtering out irrelevant noise. \n",
    "\n",
    "If the hidden layers had more neurons than the input layers, the model could become too flexible and reproduce the input exactly, including the noise, without effectively learning the significant patterns. The dimensionality reduction enforced by having fewer neurons in the hidden layers helps the model focus on the core features of the data.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/0*RR_anSGJL6W3FqCw.png\"  width=\"600\">\n",
    "\n",
    "The `encoder` compresses the input values into a more compact representation at the core layer. This process resembles a funnel that is wide on the left side, where the input data enters, and narrows towards the right side, where the compressed representation is produced.\n",
    "\n",
    "The `decoder` reconstructs the compressed information to generate the output. This process is visually similar to an inverted funnel: narrow on the left side, where the compressed data is input, and widening towards the right side, where the reconstructed output is produced. The information of the image is highly compressed in the core hidden layer then recontructed to itself. Thus the core hidden layer should retain the information of the image as much as possible.\n",
    "\n",
    "Typically, the architecture of the decoderis designed **to mirror** the encoder. This means that the number of neurons and the number of hidden layers in the decoding funnel are structured to correspond to those in the encoding funnel. Most practitioners adopt this symmetry as a standard approach. \n",
    "\n",
    "Autoencoders have various applications, like **dimensionality reduction**, **image coloring** or even **noise reduction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Neural Network Model for Binary Classification**\n",
    "\n",
    "Now that we have established the theoretical foundations regarding image classification and key concepts such as convolutional neural network architecture (CNN), we can move on to the creation of our model. In the next section, we will design and implement our binary classification model. \n",
    "\n",
    "## 1. Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DataSet Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DataSet Visualization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
